"""Model comparison and evaluation tool."""
import torch
import torch.nn as nn
import numpy as np
from PyQt5 import QtWidgets
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report
import matplotlib.pyplot as plt
import matplotlib.backends.backend_qt5agg as backend

class SimpleMLPModel(nn.Module):
    """Simple MLP for comparison."""
    def __init__(self, input_size, num_classes):
        super().__init__()
        self.fc = nn.Sequential(
            nn.Linear(input_size, 128),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(64, num_classes)
        )
        
    def forward(self, x):
        x = x.view(x.size(0), -1)  # Flatten
        return self.fc(x)

class ModelComparison(QtWidgets.QWidget):
    def __init__(self):
        super().__init__()
        self.setWindowTitle("üî¨ Model Comparison")
        self.setGeometry(100, 100, 1000, 600)
        self.setup_ui()
        
    def setup_ui(self):
        layout = QtWidgets.QVBoxLayout()
        
        # Title
        title = QtWidgets.QLabel("üî¨ EEG Model Comparison Tool")
        title.setStyleSheet("font-size: 18px; font-weight: bold; padding: 10px;")
        layout.addWidget(title)
        
        # Model selection
        model_group = QtWidgets.QGroupBox("Select Models to Compare")
        model_layout = QtWidgets.QVBoxLayout()
        
        self.cnn_check = QtWidgets.QCheckBox("CNN (Current Model)")
        self.mlp_check = QtWidgets.QCheckBox("Multi-Layer Perceptron")
        self.rf_check = QtWidgets.QCheckBox("Random Forest")
        self.svm_check = QtWidgets.QCheckBox("Support Vector Machine")
        
        self.cnn_check.setChecked(True)
        self.mlp_check.setChecked(True)
        
        model_layout.addWidget(self.cnn_check)
        model_layout.addWidget(self.mlp_check)
        model_layout.addWidget(self.rf_check)
        model_layout.addWidget(self.svm_check)
        model_group.setLayout(model_layout)
        
        # Controls
        controls = QtWidgets.QHBoxLayout()
        self.compare_btn = QtWidgets.QPushButton("üöÄ Compare Models")
        self.export_btn = QtWidgets.QPushButton("üìä Export Results")
        
        controls.addWidget(self.compare_btn)
        controls.addWidget(self.export_btn)
        controls.addStretch()
        
        # Results area
        self.results_text = QtWidgets.QTextEdit()
        self.results_text.setMaximumHeight(200)
        
        # Plot area
        self.figure = plt.Figure(figsize=(10, 6))
        self.canvas = backend.FigureCanvasQTAgg(self.figure)
        
        layout.addWidget(model_group)
        layout.addLayout(controls)
        layout.addWidget(self.results_text)
        layout.addWidget(self.canvas)
        
        self.setLayout(layout)
        
        # Connect buttons
        self.compare_btn.clicked.connect(self.compare_models)
        self.export_btn.clicked.connect(self.export_results)
        
    def compare_models(self):
        """Compare selected models."""
        # Simulate model comparison (replace with actual implementation)
        results = []
        results.append("=== MODEL COMPARISON RESULTS ===")
        results.append("")
        
        model_scores = {}
        
        if self.cnn_check.isChecked():
            # Simulate CNN results
            accuracy = np.random.uniform(0.85, 0.95)
            model_scores['CNN'] = accuracy
            results.append(f"üß† CNN Model:")
            results.append(f"   Accuracy: {accuracy:.3f}")
            results.append(f"   Training Time: ~5 minutes")
            results.append("")
            
        if self.mlp_check.isChecked():
            # Simulate MLP results
            accuracy = np.random.uniform(0.75, 0.85)
            model_scores['MLP'] = accuracy
            results.append(f"üîó Multi-Layer Perceptron:")
            results.append(f"   Accuracy: {accuracy:.3f}")
            results.append(f"   Training Time: ~2 minutes")
            results.append("")
            
        if self.rf_check.isChecked():
            # Simulate Random Forest results
            accuracy = np.random.uniform(0.70, 0.80)
            model_scores['Random Forest'] = accuracy
            results.append(f"üå≥ Random Forest:")
            results.append(f"   Accuracy: {accuracy:.3f}")
            results.append(f"   Training Time: ~1 minute")
            results.append("")
            
        if self.svm_check.isChecked():
            # Simulate SVM results
            accuracy = np.random.uniform(0.65, 0.75)
            model_scores['SVM'] = accuracy
            results.append(f"üìê Support Vector Machine:")
            results.append(f"   Accuracy: {accuracy:.3f}")
            results.append(f"   Training Time: ~3 minutes")
            results.append("")
            
        # Find best model
        if model_scores:
            best_model = max(model_scores, key=model_scores.get)
            results.append(f"üèÜ Best Model: {best_model} ({model_scores[best_model]:.3f})")
            
        self.results_text.setText('\n'.join(results))
        
        # Plot comparison
        if model_scores:
            self.plot_comparison(model_scores)
            
    def plot_comparison(self, model_scores):
        """Plot model comparison."""
        self.figure.clear()
        
        # Bar plot
        ax1 = self.figure.add_subplot(1, 2, 1)
        models = list(model_scores.keys())
        scores = list(model_scores.values())
        colors = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12'][:len(models)]
        
        bars = ax1.bar(models, scores, color=colors, alpha=0.7)
        ax1.set_ylabel('Accuracy')
        ax1.set_title('Model Accuracy Comparison')
        ax1.set_ylim(0, 1)
        
        # Add value labels on bars
        for bar, score in zip(bars, scores):
            ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                    f'{score:.3f}', ha='center', va='bottom')
                    
        # Pie chart for relative performance
        ax2 = self.figure.add_subplot(1, 2, 2)
        ax2.pie(scores, labels=models, colors=colors, autopct='%1.1f%%')
        ax2.set_title('Relative Performance')
        
        self.figure.tight_layout()
        self.canvas.draw()
        
    def export_results(self):
        """Export comparison results."""
        content = self.results_text.toPlainText()
        if not content:
            QtWidgets.QMessageBox.warning(self, "Warning", "No results to export. Run comparison first.")
            return
            
        from datetime import datetime
        filename = f"model_comparison_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
        
        try:
            with open(filename, 'w') as f:
                f.write(content)
            QtWidgets.QMessageBox.information(self, "‚úÖ Success", f"Results exported to {filename}")
        except Exception as e:
            QtWidgets.QMessageBox.critical(self, "‚ùå Error", f"Export failed: {str(e)}")

def show_model_comparison():
    """Show model comparison tool."""
    import sys
    app = QtWidgets.QApplication.instance()
    if app is None:
        app = QtWidgets.QApplication(sys.argv)
    
    comparison = ModelComparison()
    comparison.show()
    return comparison